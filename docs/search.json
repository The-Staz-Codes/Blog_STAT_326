[
  {
    "objectID": "posts/05-Hypothesis_Testing/index.html",
    "href": "posts/05-Hypothesis_Testing/index.html",
    "title": "Mini-Project #5",
    "section": "",
    "text": "Advantages and Drawbacks of Using p-values\n\nTowards the end of Section 1, the authors say, “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\n\nWith this quote, the author is trying to convey the importance of not relying as heavily on p-values to draw conclusions as modern scientists do. Worrying about whether a p-value deems an investigation “statistically significant” is superficial and retracts from the actual thought process and the experiment’s scientific context. It is analogous to a student hyperfixating on obtaining high grades and not necessarily caring about the content they are learning, which leads to it deteriorating faster over time. As less emphasis is placed on the superficial p-value, the otherwise neglected experimental components and statistical thinking behind the data analysis are brought to light.\nIn my opinion, “statistical thinking” refers to the act of designing experiments in a way that acknowledges uncertainty and provides a way to minimize the noise that ultimately comes with data collection and analysis. Examples of statistical thinking include incorporating prior information into the collection and analysis of data, such as the prior distributions in Bayesian statistics, or using prior information as a comparison for determining if a new finding provides credible evidence for a nonzero effect in an “analysis of credibility.”\n\nSection 2, third paragraph: The authors state, “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\n\nWhat the author means is that the p-value is a distinct piece of information, “the probability of obtaining a test statistic that is as or more in favor of the alternative hypothesis than the one observed assuming the null hypothesis is true,” that is unchanged by whether or not the user decides it is worth caring about. The label of ‘statistical significance’ is merely a label; it is subjective and does not add a layer of information that was not already there. The misconception that the ability to conclude anything is baked into the p-value itself is harmful because it negates scientific reasoning and uncertainty considerations. If the p-value somehow possessed this information, Type I & II errors wouldn’t exist, nor would assumptions be associated with “false positive risks of 20-30%,” according to Colquhoun.\n\nSection 2, end of first column: The authors state, “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\n\nI think the p-value is useful enough not to be omitted since it can indicate whether a question warrants further investigation; however, based on the misconceptions and the disproportionate weight p-values have come to hold, I agree with the statement. Something I immediately thought of when I first read this article, which is explicitly mentioned in section 3.3.2, is how some researchers will falsify their data or make tweaks just to obtain a smaller p-value that the community deems “significant.” Due to the competitive nature and over achievement being the standard in academia, researchers are willing to manipulate their data to obtain higher impact factors on their papers. Although some information would be lost if omitting the p-value became standard, this malpractice would no longer be an issue, which I feel is a worthy trade-off.\nIn terms of what should be presented in scientific publishing, I think it boils down to the aforementioned notions of “statistical thinking.” If a statistical test is well-designed and incorporates scientific context to maximize the information that can be derived from it, as deemed by the researcher, then it should be included.\n\nSection 3, end of page 2: The authors state, “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\n\nI agree with this statement; a universal approach to statistical inference would contradict the entire article’s rhetoric because it would eliminate the need to consider the “statistical thinking” that goes into designing or interpreting an effective experiment. The way statistical inference is utilized and how it should be interpreted depends on the scientific context and experimental structure. This is highlighted in section 3.3.2, via the quote, “we believe that the thoughtful use and interpretation of p-values will never adhere to a rigid rulebook, and will instead inevitably vary from study to study.” However, there are some core elements referenced, including assessing data quality, mechanism plausibility, and real-world benefits to an investigation, that would benefit any statistical inference approach.\n\nSection 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\n\nIn my mind, “statistical thoughtfulness” refers to the careful inclusion and communication of every necessary detail associated with an analysis. A researcher has exhibited “statistical thoughtfulness” if there is no room for misinterpretation in the results they present. As the article argues, solely depending on the p-value to determine if a statistical test is worth caring about is a gross misinterpretation, and it would be better practice to include multiple assessments in addition to (or in place of) the p-value threshold assessment. For instance, the article mentions the belief of a statistician that a confidence index should be included with every approach to statistical analysis. The article also mentions that thoughtful research: “looks ahead to prospective outcomes based on theory and previous research, carefully considers meaningful effect size, considers multiple approaches, and uses a toolbox of statistical techniques.” In other words, to practice “statistical thoughtfulness” is to utilize statistics to address a scientific question from multiple angles.\n\nSection 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence,” can be misleading, and they propose the use of “compatibility” instead. What do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\n\nWhile the terms “significance” and “confidence” can be misleading (we discussed how confidence intervals are unintuitive and the more natural interpretation aligns with credible intervals, the Bayesian analogue, and “significance” seems to assign a disproportionate amount of weight to the p-value), I don’t think their use is a major issue among the statistics community. Given the rigorous training/schooling associated with becoming a statistician, statisticians are unlikely to hold the misconceptions that could arise from the terminology. As with any field, certain customs and terminology have been widely accepted and used for a long time, which makes them resistant to change (e.g., defining the counterclockwise direction to be positive in physics and linear algebra, etc.). It is the responsibility of the person using/learning the terminology to dissect what it means and apply it properly.\nIn my opinion, the perceived problem is that, given how other fields such as biology and economics rely on statistics, someone with less rigorous statistical training may fail to understand the uncertainty associated with statistical analyses, almost as though it is masked by the terminology. While changing the name to “compatibility” could rectify this concern, the professionals in these fields are highly adaptable. I believe potential misconceptions could easily be addressed by drawing more attention to them when the material is learned/used.\n\nFind a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?\n\nA quote that stood out to me comes from the referenced statistician Goodman and appears in the second paragraph at the beginning of section 5, “It’s the same reason we can use money. When everyone believes in something’s value, we can use it for real things; money for food, and p-values for knowledge claims, publication, funding, and promotion. It doesn’t matter if the p-value doesn’t mean what people think it means; it becomes valuable because of what it buys.” I find this analogy to be particularly effective and thought-provoking because it forces the reader to consider the psychology behind what gives things “value.” Money itself is not inherently valuable; it is a green piece of paper, but the ability to use this paper to obtain desired items gives it value. Similarly, a p-value is not an indisputable indicator of whether results are worth caring about; however, it is treated as such by scientists and therefore assumes this problematic role."
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html",
    "href": "posts/03-Confidence_Intervals/index.html",
    "title": "Mini-Project #3",
    "section": "",
    "text": "Simulation to Investigate Confidence Intervals"
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html#set-up",
    "href": "posts/03-Confidence_Intervals/index.html#set-up",
    "title": "Mini-Project #3",
    "section": "Set up",
    "text": "Set up\n\n# Clear the environment\nrm(list = ls())\n\n# Load in packages \nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html#settings-for-p-close-to-0.5-p-0.56",
    "href": "posts/03-Confidence_Intervals/index.html#settings-for-p-close-to-0.5-p-0.56",
    "title": "Mini-Project #3",
    "section": "Settings for p close to 0.5 (p = 0.56)",
    "text": "Settings for p close to 0.5 (p = 0.56)\n\nFor large sample size (n = 500)\n\n# Run simulation\nn &lt;- 500   # sample size\np &lt;- 0.56  # population proportion\n\n# Define function to generate sample proportions \ngenerate_samp_prop &lt;- function(n, p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # Create 90% confidence interval \n  lb &lt;- phat - 1.645 * sqrt(phat * (1-phat) / n)\n  ub &lt;- phat + 1.645 * sqrt(phat * (1-phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\n# How many CI's do we want to generate\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n=500, p=0.56)) |&gt;\n  bind_rows()\n\n# Calculate average interval width and coverage rate\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb, \n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                              true = 1,\n                                                              false = 0))\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                                      coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0730         0.902\n\n\n\n\nFor medium sample size (n = 50)\n\n# Run simulation\nn &lt;- 50   # sample size\np &lt;- 0.56  # population proportion\n\n# Define function to generate sample proportions \ngenerate_samp_prop &lt;- function(n, p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # Create 90% confidence interval \n  lb &lt;- phat - 1.645 * sqrt(phat * (1-phat) / n)\n  ub &lt;- phat + 1.645 * sqrt(phat * (1-phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\n# How many CI's do we want to generate\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n=50, p=0.56)) |&gt;\n  bind_rows()\n\n# Calculate average interval width and coverage rate\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb, \n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                              true = 1,\n                                                              false = 0))\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                                      coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.229         0.890\n\n\n\n\nFor small sample size (n = 22)\n\n# Run simulation\nn &lt;- 22   # sample size\np &lt;- 0.56  # population proportion\n\n# Define function to generate sample proportions \ngenerate_samp_prop &lt;- function(n, p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # Create 90% confidence interval \n  lb &lt;- phat - 1.645 * sqrt(phat * (1-phat) / n)\n  ub &lt;- phat + 1.645 * sqrt(phat * (1-phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\n# How many CI's do we want to generate\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n=22, p=0.56)) |&gt;\n  bind_rows()\n\n# Calculate average interval width and coverage rate\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb, \n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                              true = 1,\n                                                              false = 0))\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                                      coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.340          0.87"
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html#settings-for-p-far-from-0.5-p-0.20",
    "href": "posts/03-Confidence_Intervals/index.html#settings-for-p-far-from-0.5-p-0.20",
    "title": "Mini-Project #3",
    "section": "Settings for p far from 0.5 (p = 0.20)",
    "text": "Settings for p far from 0.5 (p = 0.20)\n\nFor large sample size (n = 500)\n\n# Run simulation\nn &lt;- 500   # sample size\np &lt;- 0.2  # population proportion\n\n# Define function to generate sample proportions \ngenerate_samp_prop &lt;- function(n, p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # Create 90% confidence interval \n  lb &lt;- phat - 1.645 * sqrt(phat * (1-phat) / n)\n  ub &lt;- phat + 1.645 * sqrt(phat * (1-phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\n# How many CI's do we want to generate\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n=500, p=0.2)) |&gt;\n  bind_rows()\n\n# Calculate average interval width and coverage rate\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb, \n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                              true = 1,\n                                                              false = 0))\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                                      coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0588         0.898\n\n\n\n\nFor medium sample size (n = 50)\n\n# Run simulation\nn &lt;- 50   # sample size\np &lt;- 0.2  # population proportion\n\n# Define function to generate sample proportions \ngenerate_samp_prop &lt;- function(n, p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # Create 90% confidence interval \n  lb &lt;- phat - 1.645 * sqrt(phat * (1-phat) / n)\n  ub &lt;- phat + 1.645 * sqrt(phat * (1-phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\n# How many CI's do we want to generate\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n=50, p=0.2)) |&gt;\n  bind_rows()\n\n# Calculate average interval width and coverage rate\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb, \n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                              true = 1,\n                                                              false = 0))\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                                      coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.183         0.859\n\n\n\n\nFor small sample size (n = 22)\n\n# Run simulation\nn &lt;- 22  # sample size\np &lt;- 0.2  # population proportion\n\n# Define function to generate sample proportions \ngenerate_samp_prop &lt;- function(n, p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  \n  # Create 90% confidence interval \n  lb &lt;- phat - 1.645 * sqrt(phat * (1-phat) / n)\n  ub &lt;- phat + 1.645 * sqrt(phat * (1-phat) / n)\n  \n  prop_df &lt;- tibble(phat, lb, ub)\n  return(prop_df)\n}\n\n# How many CI's do we want to generate\nn_sim &lt;- 5000\n\nprop_ci_df &lt;- map(1:n_sim, \n    \\(i) generate_samp_prop(n=22, p=0.2)) |&gt;\n  bind_rows()\n\n# Calculate average interval width and coverage rate\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb, \n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                              true = 1,\n                                                              false = 0))\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                                      coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.269         0.825"
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html#check-assumptions",
    "href": "posts/03-Confidence_Intervals/index.html#check-assumptions",
    "title": "Mini-Project #3",
    "section": "Check Assumptions",
    "text": "Check Assumptions\n\n# Setting 1: p = 0.56, n = 500\n500 * 0.56\n\n[1] 280\n\n500 * (1 - 0.56)\n\n[1] 220\n\n# Result: 280 &gt; 10 and 220 &gt; 10, so large sample assumption holds\n\n# Setting 2: p = 0.56, n = 50\n50 * 0.56\n\n[1] 28\n\n50 * (1 - 0.56)\n\n[1] 22\n\n# Result: 28 &gt; 10 and 22 &gt; 10, so large sample assumption holds\n\n# Setting 3: p = 0.56, n = 22\n22 * 0.56\n\n[1] 12.32\n\n22 * (1 - 0.56)\n\n[1] 9.68\n\n# Result: 12.32 &gt; 10, but 9.68 &lt; 10, so large sample assumption is violated \n\n# Setting 4: p = 0.20, n = 500\n500 * 0.20\n\n[1] 100\n\n500 * (1 - 0.20)\n\n[1] 400\n\n# Result: 100 &gt; 10 and 400 &gt; 10, so large sample assumption holds \n\n# Setting 5: p = 0.20, n = 50\n50 * 0.20\n\n[1] 10\n\n50 * (1 - 0.20)\n\n[1] 40\n\n# Result: 10 = 10 and 40 &gt; 10, so the large sample assumption is right on the line of being violated\n\n# Setting 6: p = 0.20, n = 22\n22 * 0.20\n\n[1] 4.4\n\n22 * (1 - 0.20)\n\n[1] 17.6\n\n# Result: 17.6 &gt; 10, but 4.4 &lt; 10, so large sample assumption is violated"
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html#table-of-results",
    "href": "posts/03-Confidence_Intervals/index.html#table-of-results",
    "title": "Mini-Project #3",
    "section": "Table of Results",
    "text": "Table of Results\n\nTable of Results\n\n\n\n\n\\(n = 22\\)\n\\(n = 50\\)\n\\(n = 500\\)\n\n\n\n\n\\(p = 0.56\\)\nCoverage Rate\n0.862\n0.887\n0.905\n\n\n\\(p = 0.20\\)\nCoverage Rate\n0.839\n0.869\n0.897\n\n\n\n\n\n\n\n\n\n\\(p = 0.56\\)\nAverage Width\n0.340\n0.229\n0.0730\n\n\n\\(p = 0.20\\)\nAverage Width\n0.270\n0.183\n0.0588"
  },
  {
    "objectID": "posts/03-Confidence_Intervals/index.html#summary",
    "href": "posts/03-Confidence_Intervals/index.html#summary",
    "title": "Mini-Project #3",
    "section": "Summary",
    "text": "Summary\nThe purpose of this project was to examine confidence intervals and coverage rates across 6 different settings to assess the importance of the “large sample size” assumption. The first three settings used a proportion value close to 0.5, p = 0.56, and varied the sample sizes, n = 500 (large), n = 50 (medium), and n = 22 (small). The trend observed for confidence intervals was that the average width decreased as sample size increased, 0.340 (n = 22) &gt; 0.229 (n = 50) &gt; 0.0730 (n = 500). Mathematically, this makes sense as the formula for confidence intervals involves subtracting or adding a quantity that is divided by the square root of the sample size, which shrinks as n increases. The trend observed for coverage rates is that the larger the sample size, the closer they were to the expected 0.90, since we were calculating a 90% confidence interval, |0.90 - 0.862| = 0.038 (n = 22) &gt; |0.90 - 0.887| = 0.013 (n = 50) &gt; |0.90 - 0.905| = 0.005 (n = 500).\nThe latter three settings used a proportion value far from 0.5, p = 0.2, and varied the sample sizes by the same amounts as the first three settings, n = 500, n = 50, and n = 22. For confidence intervals, a similar trend was observed where average width decreased as sample size increased, 0.270 (n = 22) &gt; 0.183 (n = 50) &gt; 0.0588 (n = 500). A similar trend was also observed for coverage rates as larger sample sizes were closer to the expected coverage rate of 0.90, |0.90 - 0.839| = 0.061 (n = 22) &gt; |0.90 - 0.869| = 0.031 (n = 50) &gt; |0.90 - 0.897| = 0.003 (n = 500).\nBetween the different proportion values, p = 0.20, on average, always produced shorter confidence interval widths than p = 0.56 across the different population sizes. This suggests that the maximum average width occurs when the proportion is in the middle (p = 0.5) and decreases as the proportion we are examining moves to the left (closer to p = 0) or right (closer to p = 1). In general, p = 0.56 seems to produce higher coverage rates that are closer to the expected 0.9, except in the n = 500 case, where p = 0.2 was closer (0.005 vs 0.003), but not by much in each case. Sample size had a far more noticeable impact on coverage rates, with the n = 22 case producing values far from the expected 0.9 (distances of 0.038 and 0.061) and the n = 500 case producing values close to the expected 0.9 (distances of 0.003 and 0.005). Note that the “large sample size” assumption was violated for n = 20 but held for n = 500. For the case of n = 50, the assumption was on the line of being violated for p = 0.2 (producing a distance of 0.031) and held for p = 0.56 (producing a distance of 0.013). These findings suggest that measuring confidence intervals and coverage rates is only reliable or meaningful if the “large sample size” assumption holds. We want our confidence intervals to be short and our coverage rates to be close to 0.9, which we observe for n = 500, somewhat for n = 50, but not for n = 22."
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html",
    "href": "posts/01-Sampling_Distributions/index.html",
    "title": "Mini-Project #1",
    "section": "",
    "text": "Sampling Distribution of the Sample Minimum and Maximum"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#first-simulation-normal-distribution",
    "href": "posts/01-Sampling_Distributions/index.html#first-simulation-normal-distribution",
    "title": "Mini-Project #1",
    "section": "First Simulation: Normal Distribution",
    "text": "First Simulation: Normal Distribution\n\nGenerate the sample minimum\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 2   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 10.36 11.78  9.20  9.00  9.43\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n# look at the sample min\nsample_min   \n\n[1] 9\n\n\n\n\nRun simulation for sample minimum\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 9.467005\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.71\n 2  7.30\n 3  7.50\n 4  7.75\n 5  6.59\n 6  9.03\n 7  7.22\n 8  7.03\n 9  7.48\n10  8.12\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          1.91          1.72         1.31\n\n\n\n\nGenerate the sample maximum\n\nn &lt;- 5       # sample size\nmu &lt;- 10     # population mean\nsigma &lt;- 2   # population standard deviation\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rnorm(n, mu, sigma) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 10.43 10.80 10.88  8.52 10.80\n\n# compute the sample max\nsample_max &lt;- max(single_sample)\n# look at the sample max\nsample_max  \n\n[1] 10.88\n\n\n\n\nRun a simualtion for the sample maximum\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 11.76323\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  11.3\n 2  10.8\n 3  12.4\n 4  12.0\n 5  10.4\n 6  14.8\n 7  12.2\n 8  13.6\n 9  12.7\n10  11.0\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxs\",\n       title = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          18.3          1.77         1.33"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#second-simulation-uniform-distribution",
    "href": "posts/01-Sampling_Distributions/index.html#second-simulation-uniform-distribution",
    "title": "Mini-Project #1",
    "section": "Second Simulation: Uniform Distribution",
    "text": "Second Simulation: Uniform Distribution\n\nGenerate the sample minimum\n\nn &lt;- 5       # sample size\ntheta_1 &lt;- 7     # Starting point\ntheta_2 &lt;- 13   # Ending point\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- runif(n = n, min = theta_1, max = theta_2) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 12.76  7.19  9.00 12.41 10.68\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n# look at the sample min\nsample_min \n\n[1] 7.19\n\n\n\n\nRun simulation for the sample minimum\n\nn &lt;- 5       # sample size\ntheta_1 &lt;- 7     # Starting point\ntheta_2 &lt;- 13   # Ending point\n\ngenerate_samp_min &lt;- function(theta_1, theta_2, n) {\n  \n  single_sample &lt;- runif(n, theta_1, theta_2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(theta_1 = theta_1, theta_2 = theta_2, n = n)\n\n[1] 8.42404\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(theta_1 = theta_1, theta_2 = theta_2, n = n))\n\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  8.22\n 2 10.5 \n 3  7.04\n 4  7.95\n 5  7.30\n 6  7.56\n 7  7.60\n 8  9.51\n 9  7.65\n10  7.64\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          7.00         0.686        0.828\n\n\n\n\nGenerate the sample maximum\n\nn &lt;- 5       # sample size\ntheta_1 &lt;- 7     # Starting point\ntheta_2 &lt;- 13   # Ending point\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- runif(n = n, min = theta_1, max = theta_2) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 9.78 9.91 9.68 8.92 9.58\n\n# compute the sample max\nsample_max &lt;- max(single_sample)\n# look at the sample max\nsample_max \n\n[1] 9.91\n\n\n\n\nRun simulation for the sample maximum\n\nn &lt;- 5       # sample size\ntheta_1 &lt;- 7     # Starting point\ntheta_2 &lt;- 13   # Ending point\n\ngenerate_samp_max &lt;- function(theta_1, theta_2, n) {\n  \n  single_sample &lt;- runif(n, theta_1, theta_2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(theta_1 = theta_1, theta_2 = theta_2, n = n)\n\n[1] 12.30502\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(theta_1 = theta_1, theta_2 = theta_2, n = n))\n\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  12.2\n 2  12.1\n 3  12.2\n 4  12.4\n 5  12.8\n 6  11.8\n 7  11.3\n 8  12.7\n 9  11.0\n10  12.2\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxs\",\n       title = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          13.0         0.720        0.848"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#third-simualtion-exponential",
    "href": "posts/01-Sampling_Distributions/index.html#third-simualtion-exponential",
    "title": "Mini-Project #1",
    "section": "Third Simualtion: Exponential",
    "text": "Third Simualtion: Exponential\n\nGenerate the sample minimum\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5     # Value for lambda\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n = n, rate = lambda) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 0.91 0.84 1.97 3.33 1.63\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n# look at the sample min\nsample_min \n\n[1] 0.84\n\n\n\n\nRun simulation for the sample minimum\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5     # Value for lambda\n\n\ngenerate_samp_min &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(lambda = lambda, n = n)\n\n[1] 0.1526381\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(lambda = lambda, n = n))\n\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n      mins\n     &lt;dbl&gt;\n 1 0.126  \n 2 0.454  \n 3 0.00158\n 4 0.00370\n 5 0.0293 \n 6 1.42   \n 7 0.250  \n 8 0.0766 \n 9 0.0516 \n10 0.985  \n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1    0.00000460         0.150        0.387\n\n\n\n\nGenerate the sample maximum\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5     # Value for lambda\n\n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rexp(n = n, rate = lambda) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 0.33 0.88 0.02 5.54 2.07\n\n# compute the sample max\nsample_max &lt;- max(single_sample)\n# look at the sample max\nsample_max \n\n[1] 5.54\n\n\n\n\nRun simulation for the sample maximum\n\nn &lt;- 5       # sample size\nlambda &lt;- 0.5     # Value for lambda\n\n\ngenerate_samp_max &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(lambda = lambda, n = n)\n\n[1] 5.03299\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(lambda = lambda, n = n))\n\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  5.06\n 2  1.83\n 3  6.27\n 4  4.28\n 5  2.15\n 6  7.88\n 7  3.60\n 8  6.43\n 9  4.23\n10  3.75\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxs\",\n       title = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          19.1          5.88         2.43"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#fourth-simulation-beta-distribution",
    "href": "posts/01-Sampling_Distributions/index.html#fourth-simulation-beta-distribution",
    "title": "Mini-Project #1",
    "section": "Fourth Simulation: Beta Distribution",
    "text": "Fourth Simulation: Beta Distribution\n\nGenerate the sample minimum\n\nn &lt;- 5        # sample size\nalpha &lt;- 8    # Alpha value\nbeta &lt;- 2     # Beta value\n  \n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rbeta(n = n, shape1 = alpha, shape2 = beta) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 0.94 0.85 0.81 0.86 0.83\n\n# compute the sample min\nsample_min &lt;- min(single_sample)\n# look at the sample min\nsample_min \n\n[1] 0.81\n\n\n\n\nRun simulation for the sample minimum\n\nn &lt;- 5       # sample size\nalpha &lt;- 8    # Alpha value\nbeta &lt;- 2     # Beta value\n\ngenerate_samp_min &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(alpha = alpha, beta = beta, n = n)\n\n[1] 0.7044353\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_min function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(alpha = alpha, beta = beta, n = n))\n\n## print some of the 5000 mins\n## each number represents the sample min from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.622\n 2 0.815\n 3 0.754\n 4 0.699\n 5 0.452\n 6 0.673\n 7 0.631\n 8 0.815\n 9 0.619\n10 0.626\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Min when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(min_samp_dist = min(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  min_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.235        0.0113        0.106\n\n\n\n\nGenerate the sample maximum\n\nn &lt;- 5        # sample size\nalpha &lt;- 8    # Alpha value\nbeta &lt;- 2     # Beta value\n  \n# generate a random sample of n observations from a normal population\nsingle_sample &lt;- rbeta(n = n, shape1 = alpha, shape2 = beta) |&gt; round(2)\n# look at the sample\nsingle_sample \n\n[1] 0.47 0.84 0.74 0.69 0.68\n\n# compute the sample max\nsample_max &lt;- max(single_sample)\n# look at the sample max\nsample_max\n\n[1] 0.84\n\n\n\n\nRun simulation for the sample minimum\n\nn &lt;- 5       # sample size\nalpha &lt;- 8    # Alpha value\nbeta &lt;- 2     # Beta value\n\ngenerate_samp_max &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(alpha = alpha, beta = beta, n = n)\n\n[1] 0.8393105\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_max function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(alpha = alpha, beta = beta, n = n))\n\n## print some of the 5000 maxs\n## each number represents the sample max from __one__ sample.\nmaxs_df &lt;- tibble(mins)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.622\n 2 0.815\n 3 0.754\n 4 0.699\n 5 0.452\n 6 0.673\n 7 0.631\n 8 0.815\n 9 0.619\n10 0.626\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Maxs\",\n       title = paste(\"Sampling Distribution of the \\nSample Max when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(max_samp_dist = max(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  max_samp_dist var_samp_dist sd_samp_dist\n          &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1         0.999       0.00207       0.0455\n\n\n\n\nSummary Table\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n8.45\n7.7\n0.25\n0.578\n\n\n\\(\\text{E}(Y_{max})\\)\n16.51\n12.58\n4.10\n0.906\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n2.68\n0.849\n0.40\n0.105\n\n\n\\(\\text{SE}(Y_{max})\\)\n2.67\n0.848\n2.39\n0.046\n\n\n\n\n\nQuestion 2:"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#pdf-for-min",
    "href": "posts/01-Sampling_Distributions/index.html#pdf-for-min",
    "title": "Mini-Project #1",
    "section": "PDF for Min",
    "text": "PDF for Min\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 3, length.out = 1000)\n\n## PDF denstity\ndensity &lt;- n * exp(-(1/2) * x)^4 * 0.5 * exp(-(1/2) * x)\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#pdf-for-max",
    "href": "posts/01-Sampling_Distributions/index.html#pdf-for-max",
    "title": "Mini-Project #1",
    "section": "PDF for Max",
    "text": "PDF for Max\n\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 15, length.out = 1000)\n\n## PDF denstity\ndensity &lt;- n * (-exp(-(1/2) * x) + 1)^4 * 0.5 * exp(-(1/2) * x)\n\n\n## put into tibble and plot\nsamp_max_df &lt;- tibble(x, density)\nggplot(data = samp_max_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/01-Sampling_Distributions/index.html#questions",
    "href": "posts/01-Sampling_Distributions/index.html#questions",
    "title": "Mini-Project #1",
    "section": "Questions",
    "text": "Questions\n\nThe values for \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) closely resemble one another under both the normal and uniform distribution models, which are symmetric. However, they differ greatly under the exponential and beta distribution models, which are unsymmetric. Thus, in general \\(SE(Y_{min})\\) ≠ \\(SE(Y_{max})\\) for an unsymmetric distribution model and \\(SE(Y_{min})\\) ≃ \\(SE(Y_{max})\\) for a symmetric distribution model.\nThe values for \\(SE(Y_{min})\\) and \\(E(Y_{max})\\) between the simulation and theoretical calculations are very similar to one another. The \\(E(Y_{min})\\)) and \\(E(Y_{max})\\) are also similar, but not as closely as the standard error comparisons, which makes sense since these values are more influenced by the varying individual values in a generated sampling distribution. In other words, running the simulation multiple times results in a standard error resembling the theoretical calculations and an expectation relatively close, but often noticeably above or below its theoretical counterpart."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mathematical Statistics Mini-Projects",
    "section": "",
    "text": "Mini-Project #5\n\n\n\n\n\n\n\n\n\n\n\nJason Stasio\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project #4\n\n\n\n\n\n\n\n\n\n\n\nJason Stasio\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project #3\n\n\n\n\n\n\n\n\n\n\n\nJason Stasio\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project #2\n\n\n\n\n\n\n\n\n\n\n\nJason Stasio\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project #1\n\n\n\n\n\n\n\n\n\n\n\nJason Stasio\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/02-Estimation/index.html",
    "href": "posts/02-Estimation/index.html",
    "title": "Mini-Project #2",
    "section": "",
    "text": "Meaningful Story\nThe year is 2125 and almost everyone you pass on the street has opted to install some form of technological enhancement into their arms. This has allowed individuals to instantly acquire skills like playing the guitar or conquering challenging video game bosses without investing the time required to build them. The arms are programmable and perform designated movements for you.\nWhile miracles, these programmable arms are only as capable as the users operating them. One faithful user, subject 4150, realizes this as he hovers the input panel on his forearm over his puzzled face, illuminated by the screen’s glow. He glances away toward the three yellow balloons that shout their existence among a sea of blue balloons all affixed to a withered wooden wall. He must throw three darts and pop all three yellow balloons to win the ginormous stuffed capybara sitting atop the carnival stall. It’s lifeless and unbothered expression is oddly soothing.\nUnimpressed with his panicked composure, his date glances over his shoulder and sees his screen displays two estimators that can be used to predict the location of where his dart throw will land. She realizes that the location of his dart throw is a parameter she can visualize with her technologically enhanced eyes, a new development many people in society have yet to adopt. In fact, she can visualize a simulation of the dart landing as though it were really happening many times.\nShe runs a simulation of subject 4150’s dart throws, which follow an exponential distribution, 1000 times for each of the yellow balloons. She treats each throw to one of the yellow balloons as a random variable and, assuming independence, decides to take random samples from each distribution, investigating the mean of where the dart lands. She takes several random samples of size 5 and plugs the results through each of the estimators on subject 4150’s screen.\nShe finds that the first estimator, \\(\\mu_{1}\\), produces an estimate that is slightly biased toward the top left of where the true mean throw is. The second estimator, \\(\\mu_{2}\\), is unbiased but has a much larger variance or range of balloons popped. Realizing choosing which estimator to help him pop the yellow balloons is not as straightforward as she thought, she decides to lift some of the judgment she had cast on subject 4150.\nShe turns to the mean square error metric to help assess which estimator subject 4150 should use. Despite being biased, \\(\\mu_{1}\\) produces a smaller mean square error and thus can be thought of as the better estimator, a prime example of the bias-variance trade-off.\nHowever, unsatisfied with her results and determined to take the stuffed capybara home, she realizes subject 4150’s enhancements are slightly outdated. She decides to utilize her brain enhancements, the data from her simulations, and the fact she knows subject 4150’s throws will follow an exponential dis- tribution to mentally derive a new estimator, \\(\\mu_{3}\\), using a maximum likelihood function.\nShe then analyzes \\(\\mu_{3}\\) and finds it to be unbiased like \\(\\mu_{2}\\), but with a much lower variance. In fact, if she increases the sample size of her random samples, the variance converges to 0, meaning \\(\\mu_{3}\\) is also a consistent estimator. Of the estimators, \\(\\mu_{3}\\) proves to be the best as it produces the smallest mean square error, so she sends the information to subject 4150’s enhancements.\nThe new estimator pops up on his screen and without questioning, he uses it as a guide for how he should program his darts throws to maximize his chances of hitting the yellow balloons. To his pleasant surprise, he pops all three of the yellow balloons flawlessly.\nThe happy pair walk away from the stall, giant stuffed capybara in hand."
  },
  {
    "objectID": "posts/04-Bayesian_Statistics/index.html",
    "href": "posts/04-Bayesian_Statistics/index.html",
    "title": "Mini-Project #4",
    "section": "",
    "text": "Bayesian Analysis"
  },
  {
    "objectID": "posts/04-Bayesian_Statistics/index.html#introduction",
    "href": "posts/04-Bayesian_Statistics/index.html#introduction",
    "title": "Mini-Project #4",
    "section": "Introduction",
    "text": "Introduction\nThe goal of this project is to apply a Bayesian approach to modeling the probability that Rafael Nadal, one of the world’s greatest men’s clay-court tennis players, wins a point on his own serve against his primary rival, Novak Djokovic, during the French open tournament. It investigates how the data transforms three different scenarios/prior distributions, a noninfoirmative prior where we know nothing about the probability of Nadal scoring, an informative prior based on a previous clay-court match where Nadal won 46 out of 66 points with standard error 0.05657, and an informative prior based on a sports announcer’s claim that Nadal wins the point about 75% of the time and never less than 70% of the time. First, the prior distributions are determined and graphed, with justification for choices included along with the code. Next, the data is applied to the prior to create the posterior distributions, which are determined and graphed. Calculations associated with these determinations are also included in the body using built-in latex. Along with graphing the posterior distributions, their means and associated 90% credible intervals are determined, with the method used to obtain these values given. Finally, a comparison and conclusion will be conducted to interpret the results."
  },
  {
    "objectID": "posts/04-Bayesian_Statistics/index.html#body",
    "href": "posts/04-Bayesian_Statistics/index.html#body",
    "title": "Mini-Project #4",
    "section": "Body",
    "text": "Body\n\nSet-up\n\n# Clear the environment\nrm(list = ls())\n\n# Load in packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\n\nPrior Distributions\n\nps &lt;- seq(0, 1, length.out = 1000)\n\n# Prior 1: Non-informative\n\n# We will use a beta distribution with alpha=1 and beta=1 (equivalent to uniform(0,1)) to represent our non-informative prior since it is flat and thus any value between 0 and 1 holds equal density.\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\nnoninformative_prior &lt;- dbeta(ps, noninformative_alpha, noninformative_beta)\n\n# Prior 2: Informative Based on a Clay-court Match\n\n# Since we are given the number of points Nadal wins on his serve against Djokovic out of the total number of points for his serves, we have a proportion represented as a fraction that can take on values between 0 and 1. Thus, a beta distribution is a good choice for the prior. The general mean of the beta distribution can be set to the information given, a fraction, to solve for alpha and beta parameters. Let us note that we could also set it equal to the proportion represented as a decimal, solve for the general beta value depending on alpha, sequence alpha, then look for the values that give a standard error close to the one given. However, this would be unnecessary and computationally inefficient since we are given an exact value for the mean, which allows us to directly solve for specific alpha and beta parameters representative of the scenario.\n\n\\[ \\frac{46}{66} = \\frac{\\alpha}{\\alpha +\\beta} \\implies \\alpha = 46,\\text{ } \\beta +\\alpha = 66 \\implies \\beta = 66 - \\alpha= 66 - 46=20\\]\n\nccm_informative_alpha &lt;- 46\nccm_informative_beta &lt;- 20\n\nccm_informative_prior &lt;- dbeta(ps, ccm_informative_alpha, ccm_informative_beta)\n\n# Prior 3: Informative Based on Sports Announcer\n\n# Once again, we are given information pertaining to the proportion of points Nadal wins on his serves against Djokovic, so a beta distribution is a good choice for the prior. The sports announcer claims Nadal wins 75% of the points on his serve against Djokovic, which corresponds to the proportion 0.75. He also says he is \"almost\" sure Nadal wins no less than 70% of his serves, so we can adopt the strategy of solving for the beta parameter depending on alpha, sequencing through alpha values, calculating the distance to having a probability of 0.02 (a small value) of being less than 0.7 based on the parameters and filtering to find the smallest one. The parameters associated with the smallest distance to our target probability will be used. \n\n\\[0.75 = \\frac{\\alpha}{\\alpha + \\beta}\\implies 0.75\\alpha +0.75\\beta=\\alpha\\implies0.75\\beta=0.25\\alpha \\implies \\beta =\\frac{0.25\\alpha}{0.75} \\]\n\nalphas &lt;- seq(1, 1000, length.out = 5000)\nbetas &lt;- (0.25 * alphas) / 0.75\n\ntarget_prob &lt;- 0.02\nprob_less_2 &lt;- pbeta(0.7, alphas, betas)\n\ntibble(alphas, betas, prob_less_2) |&gt;\n  mutate(close_to_target = abs(prob_less_2 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_2 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;           &lt;dbl&gt;\n1   252.  83.9      0.0200     0.000000320\n\nsann_informative_alpha &lt;- 252\nsann_informative_beta &lt;- 83.9\n\nsann_informative_prior &lt;- dbeta(ps, sann_informative_alpha, sann_informative_beta)\n\nplot_df_prior &lt;- tibble(ps, noninformative_prior,\n                  ccm_informative_prior,\n                  sann_informative_prior) |&gt;\n  pivot_longer(cols = -ps, names_to = \"distribution\", values_to = \"density\")\n\nggplot(data = plot_df_prior, aes(x = ps, y = density, colour = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\n\n\n\n\n\n\n\n\n\n\nData\nSince we are given data that has two outcomes, either Nadal scored the point on his serve or he did not, we can model it as a binomial distribution. We proved in class that combining a prior beta distribution with a binomial distribution will result in a posterior beta distribution with parameters:\n\\[\\alpha_{post} = y_{obs} + \\alpha, \\text{ } \\\\ \\beta_{post} = n - y_{obs} + \\beta\\] Now, we can solve for the posterior distribution for each setting using the fact Nadal won 56 of the points from 84 possible points.\nNon-informative: \\[ \\alpha_{post} = 56 + 1=57, \\text{ } \\\\ \\beta_{post} = 84 - 56+1=29\\]\nInformative based on clay-court match: \\[\\alpha_{post} = 56 + 46=102, \\text{ } \\\\ \\beta_{post} = 84 - 56+20=48\\]\nInformative based on sports announcer: \\[\\alpha_{post} = 56 + 252 = 308, \\text{ } \\\\ \\beta_{post} = 84 - 56 + 83.9=111.9\\]\n\n# Plot the new posterior distributions\nnoninformative_posterior = dbeta(ps, 57, 29)\nccm_informative_posterior = dbeta(ps, 102, 48)\nsann_informative_posterior = dbeta(ps, 308, 111.9)\n\nplot_df_post &lt;- tibble(ps, noninformative_posterior,\n                  ccm_informative_posterior,\n                  sann_informative_posterior) |&gt;\n  pivot_longer(cols = -ps, names_to = \"distribution\", values_to = \"density\")\n\nggplot(data = plot_df_post, aes(x = ps, y = density, colour = distribution)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p\")\n\n\n\n\n\n\n\n\nNow, let us calculate the means of the posterior distributions.\nNon-informative: \\[\\frac{\\alpha_{post}}{\\alpha_{post} + \\beta_{post}} = \\frac{57}{57+29}\\approx 0.663\\]\nInformative based on clay-court match: \\[\\frac{\\alpha_{post}}{\\alpha_{post} + \\beta_{post}} = \\frac{102}{102+48} = 0.68 \\]\nInformative based on sports announcer: \\[\\frac{\\alpha_{post}}{\\alpha_{post} + \\beta_{post}} = \\frac{308}{308 + 111.9}\\approx 0.734 \\]\nFurthermore, let us find 90% credible intervals for each of the scenarios.\nNon-informative\n\nqbeta(c(0.05,0.95), 57, 29)\n\n[1] 0.5772453 0.7440061\n\n# According to our model, there is a 90% probability that the proportion of Nadal's serves where he wins the point is between 0.5772453 and 0.7440061\n\nInformative based on clay-court match:\n\nqbeta(c(0.05,0.95), 102, 48)\n\n[1] 0.6161904 0.7410715\n\n# According to our model, there is a 90% probability that the proportion of Nadal's serves where he wins the point is between 0.6161904 and 0.7410715\n\nInformative based on sports announcer\n\nqbeta(c(0.05,0.95), 308, 111.9)\n\n[1] 0.6974328 0.7683171\n\n# According to our model, there is a 90% probability that the proportion of Nadal's serves where he wins the point is between 0.6974328 and 0.7683171"
  },
  {
    "objectID": "posts/04-Bayesian_Statistics/index.html#comparison",
    "href": "posts/04-Bayesian_Statistics/index.html#comparison",
    "title": "Mini-Project #4",
    "section": "Comparison",
    "text": "Comparison\nNotice, the three posterior distributions are different from one another. This makes sense since they all follow beta distributions but with different parameters. In each setting, the beta parameter is smaller than the alpha parameter, but the distance between them varies. This is reflected by the fact the distributions have different variances, with smaller distances between parameters corresponding to larger variances. The posterior distribution that takes into account the previous clay-court match’s data appears the most unchanged from its original prior distribution counterpart in terms of center, and it also has the second smallest variance. This suggests that this prior was in the closest alignment to the observed data, which would also be another explanation as to why it has the second smallest variance because the values are closer to the target compared to the noninformative prior. The prior and posterior distributions based on the sport’s announcer’s claims have the smallest variance, but this is likely due to bias in his reporting as we can see its center differs between the prior and posterior distributions to a greater degree compared to the other informative distribution setting. Thus, if a scenario had to be chosen, it would make sense to chose the prior distribution that takes into account the information given by the previous clay-court match data because it is the most unshifted from its center and has a relatively small variance that is not a result of bias. It also makes intuitive sense considering we have knowledge about previous matches, ruling out the need for a non-informative prior, and this informative prior takes into account data rather than opinion (which is more subject to bias)."
  },
  {
    "objectID": "posts/04-Bayesian_Statistics/index.html#conclusion",
    "href": "posts/04-Bayesian_Statistics/index.html#conclusion",
    "title": "Mini-Project #4",
    "section": "Conclusion",
    "text": "Conclusion\nAfter analyzing the three scenarios, we found that the prior distribution based on the data from the previous clay-court match where Nadal won 46 out of 66 points with standard error 0.05657, is the best choice because it produces the posterior distribution with the second smallest variance and has the smallest shift from its center. A comparison between prior and posterior distributions shows that the sports announcer’s setting is likely biased, but comes with the benefit of a smaller variance (e.g. bias-variance trade-off), while the other settings are unbiased and the informative prior has a less dramatic change than the non-informative, suggesting it is in closer alignment with the data. The mean of the posterior distribution for this setting is 0.68 and produces a 90% credible interval of (0.6161904 and 0.7410715). Thus, according to our model and Bayesian approach, there is a 90% probability that the proportion of Nadal’s serves where he wins the point against his rival, Djokovic, during the French open tournament is between 0.6161904 and 0.7410715."
  }
]